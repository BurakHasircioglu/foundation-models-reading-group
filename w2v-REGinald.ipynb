{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_splitter import split_text_into_sentences\n",
    "\n",
    "def sentences_split(text: str) -> list:\n",
    "    sentences = split_text_into_sentences(text, language=\"en\")\n",
    "    return sentences\n",
    "\n",
    "df = pd.read_csv(\"../generate_toponym_dataset/1880-1900-LwM-HMD-subsample.csv\")\n",
    "\n",
    "df = df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4830\n",
      "['CHAPTER', 'A', 'few', 'years', 'before', 'the', 'date', 'on', 'which', 'this', '\"narrative', 'opens', 'there', 'had', 'come', 'to', 'Burslem', 'from', 'nother', 'part', 'of', 'the', 'Midlands', 'a', 'young', 'medical', 'gentleman,', 'fresh', 'from', 'his', 'professional', 'studies,', 'and', 'from', 'the', 'cultured', 'teaching', 'of', 'Rugby', 'public', \"'school.\"]\n"
     ]
    }
   ],
   "source": [
    "texts = df['text'].dropna().to_list()\n",
    "sentences = [sent.split(\" \") for text in texts for sent in sentences_split(text)]\n",
    "print (len(sentences))\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input, seq_length, model):\n",
    "    seq = input.copy()\n",
    "    output = input.copy()\n",
    "    for i in range(seq_length):\n",
    "        words = model.predict_output_word(input, topn=len(output)+1)\n",
    "        \n",
    "        # excluding repetitions\n",
    "        word = [word[0] for word in words if word[0] not in output][0]\n",
    "\n",
    "        output.append(word)\n",
    "        seq.append(word)\n",
    "\n",
    "        #keeping the last 4 words in the input\n",
    "        seq.remove(seq[0])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Size of the word vectors\n",
    "vector_size = 50\n",
    "\n",
    "# Minimum count for words to be included in the vocabulary\n",
    "min_count = 1\n",
    "\n",
    "# Number of threads to use for training the model\n",
    "workers = 3\n",
    "\n",
    "# Context window size\n",
    "window = 5\n",
    "\n",
    "# Number of iterations over the corpus\n",
    "epochs = 100\n",
    "\n",
    "# Use skip-gram model with negative sampling and softmax activation\n",
    "sg = 1\n",
    "ns = 5\n",
    "alpha = 0.03\n",
    "\n",
    "basic_w2v = Word2Vec(sentences,vector_size=vector_size, window=window, min_count=min_count, workers=workers, epochs=epochs, sg=sg, negative=ns, alpha=alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'capital',\n",
       " 'of',\n",
       " 'England',\n",
       " 'being',\n",
       " 'spoke',\n",
       " 'man',\n",
       " 'spend',\n",
       " 'point',\n",
       " 'has',\n",
       " 'position',\n",
       " 'which',\n",
       " 'had',\n",
       " 'are']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input = ['The', 'capital', 'of', 'England']\n",
    "seq_length = 10\n",
    "\n",
    "generate_text(text_input, seq_length, basic_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8797578, 10829900)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# https://datascience.stackexchange.com/questions/97568/fine-tuning-pre-trained-word2vec-model-with-gensim-4-0\n",
    "\n",
    "finetunded_w2v = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg, negative=ns, alpha=alpha)\n",
    "\n",
    "finetunded_w2v.build_vocab(sentences)\n",
    "\n",
    "finetunded_w2v.wv.vectors_lockf = np.ones(len(finetunded_w2v.wv))\n",
    "\n",
    "finetunded_w2v.wv.intersect_word2vec_format('glove.6B.50d.word2vec.txt', binary=False)\n",
    "\n",
    "finetunded_w2v.train(sentences, total_examples=len(sentences),  epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'capital',\n",
       " 'of',\n",
       " 'England',\n",
       " 'starch',\n",
       " '+',\n",
       " 'duties',\n",
       " 'south',\n",
       " 'Cory',\n",
       " 'reservoirs,',\n",
       " 'members.',\n",
       " 'tons;',\n",
       " 'station',\n",
       " 'supervision']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(text_input, seq_length, finetunded_w2v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
