{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ea7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "seed = 1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeea2d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f054ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check torch is using GPU acceleration\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "513658fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e9f193d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69a8d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct = True\n",
    "\n",
    "punctuation_dict = {\n",
    "    \"...\": \"||elipsis||\",\n",
    "    \".\": \"||period||\",\n",
    "    \",\": \"||comma||\",\n",
    "    \"\\\"\": \"||quotation||\",\n",
    "    \";\": \"||semicolon||\",\n",
    "    \"!\": \"||exclamation||\",\n",
    "    \"?\": \"||question||\",\n",
    "    \"(\": \"||lparathensis||\",\n",
    "    \")\": \"||rparathensis||\",\n",
    "    \"[\": \"||lsquare||\",\n",
    "    \"]\": \"||rsquare||\",\n",
    "    \"-\": \"||dash||\",\n",
    "    \"'\": \"\",\n",
    "    \"\\n\": \"\",\n",
    "    \"$\": \"||dollar||\"\n",
    "}\n",
    "\n",
    "def clean_text(text, remove_punct):\n",
    "    text = text.lower().replace(\"'\", \"\").replace(\"\\n\", \" \")\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' '+punctuation+' ')\n",
    "#     if remove_punct:\n",
    "#         for punctuation in string.punctuation:\n",
    "#             text = text.replace(punctuation, '')\n",
    "#     else:\n",
    "#         for punct in punctuation_dict.keys():\n",
    "#             text = text.replace(punct, \" \"+punctuation_dict[punct]+\" \")\n",
    "    text = ''.join([char for char in text\n",
    "                    if char in string.ascii_lowercase+string.punctuation+' '])\n",
    "    text = re.sub('  +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def read_script(filename, remove_punct=True):\n",
    "    with open(filename, 'r') as f:\n",
    "        contents = f.read()\n",
    "    return clean_text(contents, remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ea4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"GoT\"\n",
    "scripts = [read_script(os.path.join(dirname, script),\n",
    "                       remove_punct=remove_punct)\n",
    "           for script in sorted(os.listdir(dirname))]\n",
    "corpus = ' '.join(scripts)\n",
    "tokens = corpus.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959ed863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prologue the comets tail spread across the dawn , a red slash that bled above the crags of dragonstone like a wound in the pink and purple sky . the maester stood on the windswept balcony outside his '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a775e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prologue',\n",
       " 'the',\n",
       " 'comets',\n",
       " 'tail',\n",
       " 'spread',\n",
       " 'across',\n",
       " 'the',\n",
       " 'dawn',\n",
       " ',',\n",
       " 'a']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eecbeab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    Couted_text= Counter(text)\n",
    "    sorted_vocab = sorted(Couted_text,\n",
    "                          key=Couted_text.get,\n",
    "                          reverse=True)\n",
    "    \n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c396513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24114\n",
      "24114\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_to_int))\n",
    "print(len(int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85f97e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'{dirname}_token_emb.pkl', 'r') as f:\n",
    "#     token_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc2eee19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pretrained word embeddings from spaCy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_59490/3373281572.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  token_embeddings = torch.tensor([doc.vector for doc in docs])\n"
     ]
    }
   ],
   "source": [
    "use_pretrained = True\n",
    "cbow = False\n",
    "\n",
    "if use_pretrained:\n",
    "    print(\"using pretrained word embeddings from spaCy...\")\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    docs = list(nlp.pipe(tokens, n_process=6))\n",
    "    token_embeddings = torch.tensor([doc.vector for doc in docs])\n",
    "else:\n",
    "    print(\"training word2vec...\")\n",
    "    import gensim\n",
    "    word2vec = gensim.models.Word2Vec(sentences=[tokens],\n",
    "                                      vector_size=300,\n",
    "                                      window=5,\n",
    "                                      workers=6,\n",
    "                                      sg=1,\n",
    "                                      min_count=1,\n",
    "                                      seed=seed)\n",
    "    word2vec.train([tokens],\n",
    "                   total_examples=word2vec.corpus_count,\n",
    "                   epochs=50)\n",
    "    token_embeddings = torch.tensor([word2vec.wv[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2792c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dirname}_token_emb.pkl', 'wb') as f:\n",
    "    pickle.dump(token_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2329f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_pretrained:\n",
    "    print(len(word2vec.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a0791dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2084563, 300])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f49ebb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prologue',\n",
       " 'the',\n",
       " 'comets',\n",
       " 'tail',\n",
       " 'spread',\n",
       " 'across',\n",
       " 'the',\n",
       " 'dawn',\n",
       " ',',\n",
       " 'a',\n",
       " 'red',\n",
       " 'slash']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "416c1649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3344,  1.6953,  4.4473,  ..., -5.4445, -4.2309, -4.4592],\n",
       "        [-5.1043,  2.3496,  3.2472,  ..., -7.6875, -2.5128,  0.6934],\n",
       "        [-3.3126,  1.2278, -6.2548,  ..., -3.9095, -2.8027, -0.2607],\n",
       "        ...,\n",
       "        [-9.3629,  9.2761, -7.2708,  ...,  2.6801, -6.8160,  3.5737],\n",
       "        [-6.9878,  1.1615, -8.0692,  ..., -0.3003, -3.9543, -2.6215],\n",
       "        [-2.4895,  1.0770, -4.3209,  ..., -1.6725,  3.4914, -1.5722]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings[:12,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41b7122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(x_data,\n",
    "                  y_data,\n",
    "                  train_size = 0.8,\n",
    "                  batch_size = 64,\n",
    "                  seed = 42):\n",
    "    # first split data into train set, test/valid set\n",
    "    train_index, test_index = train_test_split(range(len(y_data)),\n",
    "                                               test_size=(1-train_size),\n",
    "                                               shuffle=True,\n",
    "                                               random_state=seed)\n",
    "    \n",
    "    x_train = x_data[train_index]\n",
    "    y_train = y_data[train_index]\n",
    "    x_test = x_data[test_index]\n",
    "    y_test = y_data[test_index]\n",
    "    \n",
    "    train = TensorDataset(x_train, y_train)\n",
    "    test = TensorDataset(x_test, y_test)\n",
    "    train_loader = DataLoader(dataset=train,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a4738",
   "metadata": {},
   "source": [
    "## Fixed-window model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdf11588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_for_Bengio(window_size, tokens, token_embeddings, vocab_to_int):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(window_size-1, len(tokens)):\n",
    "        # obtain word embeddings for the previous (window_size-1) words\n",
    "        X.append(token_embeddings[i-(window_size-1):i,].flatten())\n",
    "        # obtain token_id for the current word\n",
    "        y.append(vocab_to_int[tokens[i]])\n",
    "    return torch.stack(X), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a118e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2084561, 600])\n",
      "torch.Size([2084561])\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "train_size = 0.8\n",
    "batch_size = 64\n",
    "X, y = create_dataset_for_Bengio(window_size, tokens, token_embeddings, vocab_to_int)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "train_loader, test_loader = split_dataset(x_data=X,\n",
    "                                          y_data=y,\n",
    "                                          train_size=train_size,\n",
    "                                          batch_size=batch_size,\n",
    "                                          seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53a883b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bengio_et_al_word2vec(nn.Module):\n",
    "    def __init__(self, n, embedding_dim, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear((n-1)*embedding_dim, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56867632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengio_et_al_word2vec(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=600, out_features=300, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=300, out_features=24114, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = token_embeddings.shape[1]\n",
    "hidden_dim = int((window_size-1)*embedding_dim/2)\n",
    "set_seed(seed)\n",
    "model = Bengio_et_al_word2vec(n=window_size,\n",
    "                              embedding_dim=embedding_dim,\n",
    "                              hidden_dim=hidden_dim,\n",
    "                              vocab_size=len(vocab_to_int)).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cdc4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"perplexity: {math.exp(loss)}\")\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_perp, test_loss, correct = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y).item()\n",
    "            test_loss += loss\n",
    "            test_perp += math.exp(loss)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    test_perp /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test:\\n - Accuracy: {(100*correct):>0.1f}\\n\"\n",
    "          f\"- Avg loss: {test_loss:>8f}\\n\",\n",
    "          f\"- Avg perplexity: {test_perp}\\n\")\n",
    "    \n",
    "def train_and_test(model, epochs, train_loader, test_loader, loss_fn, optimizer):\n",
    "    for t in range(0, epochs+1, 500000):\n",
    "        print(f\"Epoch {t}\\n-------------------------------\")\n",
    "        train(train_loader, model, loss_fn, optimizer)\n",
    "        test(test_loader, model, loss_fn)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "263cc172",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "perplexity: 25382.982776132405\n",
      "loss: 10.141834  [   64/1667648]\n",
      "perplexity: 216.67187519881273\n",
      "loss: 5.378384  [640064/1667648]\n",
      "perplexity: 160.79597078654174\n",
      "loss: 5.080136  [1280064/1667648]\n",
      "Test:\n",
      " - Accuracy: 14.4%\n",
      "- Avg loss: 5.627925\n",
      " - Avg perplexity: 330.31095512090286\n",
      "\n",
      "Epoch 500000\n",
      "-------------------------------\n",
      "perplexity: 133.00092244079445\n",
      "loss: 4.890356  [   64/1667648]\n",
      "perplexity: 170.73113150408014\n",
      "loss: 5.140090  [640064/1667648]\n",
      "perplexity: 154.4577088973685\n",
      "loss: 5.039920  [1280064/1667648]\n",
      "Test:\n",
      " - Accuracy: 14.3%\n",
      "- Avg loss: 5.770628\n",
      " - Avg perplexity: 430.07917302528324\n",
      "\n",
      "Epoch 1000000\n",
      "-------------------------------\n",
      "perplexity: 134.1252610186134\n",
      "loss: 4.898774  [   64/1667648]\n",
      "perplexity: 226.3375768967899\n",
      "loss: 5.422028  [640064/1667648]\n",
      "perplexity: 121.6579892700085\n",
      "loss: 4.801214  [1280064/1667648]\n",
      "Test:\n",
      " - Accuracy: 14.2%\n",
      "- Avg loss: 5.872339\n",
      " - Avg perplexity: 550.3235868151647\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000000\n",
    "train_and_test(model=model,\n",
    "               epochs=epochs,\n",
    "               train_loader=train_loader,\n",
    "               test_loader=test_loader,\n",
    "               loss_fn=loss_fn,\n",
    "               optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9ec6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input, model, steps, window_size, seed=42, T=1):\n",
    "    set_seed(seed)\n",
    "    device = \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    text = clean_text(input, remove_punct)\n",
    "    print(f\"initial input text: {text}\")\n",
    "    text_tokens = text.split(' ')\n",
    "    if len(text_tokens) < window_size-1:\n",
    "        text_tokens = ['' for i in range(window_size-1)] + text_tokens\n",
    "    docs = list(nlp.pipe(text_tokens))\n",
    "    token_embeddings = torch.tensor([doc.vector for doc in docs])\n",
    "    emb_size = token_embeddings.shape[1]\n",
    "    input_vec = token_embeddings[token_embeddings.shape[0]-(window_size-1):,].flatten().to(device)\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(steps):\n",
    "            pred = model(input_vec)\n",
    "            prob = torch.pow(softmax(pred), 1/T)\n",
    "            prob = prob/prob.sum()\n",
    "            word = int_to_vocab[random.choices(list(range(len(prob))), weights=prob)[0]]\n",
    "            print(\"input tokens: \"\n",
    "                  f\"{text_tokens[token_embeddings.shape[0]-(window_size-1)+i:]}\"\n",
    "                  \" || next word:\",\n",
    "                  f\"{word}\")\n",
    "            text_tokens.append(word)\n",
    "            new_word_embedding = torch.tensor(nlp(word).vector).to(device)\n",
    "            input_vec = torch.concatenate((input_vec[emb_size:], new_word_embedding), 0)\n",
    "    return ' '.join(text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090bbf78",
   "metadata": {},
   "source": [
    "Try different values of $T$ remembering that:\n",
    "- $T$ small corresponds to just picking the most likely word\n",
    "- $T$ large corresponds to random choice over the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d8b7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = \"meanwhile in winterfell, jon and catelyn walk to\"\n",
    "# input_sequence = \"morning tony, can you take meadow to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "904005e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial input text: meanwhile in winterfell , jon and catelyn walk to\n",
      "input tokens: ['walk', 'to'] || next word: to\n",
      "input tokens: ['to', 'to'] || next word: his\n",
      "input tokens: ['to', 'his'] || next word: teeth\n",
      "input tokens: ['his', 'teeth'] || next word: about\n",
      "input tokens: ['teeth', 'about'] || next word: no\n",
      "input tokens: ['about', 'no'] || next word: notice\n",
      "input tokens: ['no', 'notice'] || next word: .\n",
      "input tokens: ['notice', '.'] || next word: those\n",
      "input tokens: ['.', 'those'] || next word: secret\n",
      "input tokens: ['those', 'secret'] || next word: whispers\n",
      "input tokens: ['secret', 'whispers'] || next word: .\n",
      "input tokens: ['whispers', '.'] || next word: as\n",
      "input tokens: ['.', 'as'] || next word: well\n",
      "input tokens: ['as', 'well'] || next word: .\n",
      "input tokens: ['well', '.'] || next word: you\n",
      "input tokens: ['.', 'you'] || next word: fell\n",
      "input tokens: ['you', 'fell'] || next word: had\n",
      "input tokens: ['fell', 'had'] || next word: done\n",
      "input tokens: ['had', 'done'] || next word: or\n",
      "input tokens: ['done', 'or'] || next word: else\n",
      "input tokens: ['or', 'else'] || next word: .\n",
      "input tokens: ['else', '.'] || next word: your\n",
      "input tokens: ['.', 'your'] || next word: lord\n",
      "input tokens: ['your', 'lord'] || next word: father\n",
      "input tokens: ['lord', 'father'] || next word: ,\n",
      "input tokens: ['father', ','] || next word: lord\n",
      "input tokens: [',', 'lord'] || next word: of\n",
      "input tokens: ['lord', 'of'] || next word: casterly\n",
      "input tokens: ['of', 'casterly'] || next word: and\n",
      "input tokens: ['casterly', 'and'] || next word: the\n",
      "input tokens: ['and', 'the'] || next word: solar\n",
      "input tokens: ['the', 'solar'] || next word: behind\n",
      "input tokens: ['solar', 'behind'] || next word: him\n",
      "input tokens: ['behind', 'him'] || next word: .\n",
      "input tokens: ['him', '.'] || next word: once\n",
      "input tokens: ['.', 'once'] || next word: it\n",
      "input tokens: ['once', 'it'] || next word: is\n",
      "input tokens: ['it', 'is'] || next word: hurting\n",
      "input tokens: ['is', 'hurting'] || next word: so\n",
      "input tokens: ['hurting', 'so'] || next word: was\n",
      "input tokens: ['so', 'was'] || next word: too\n",
      "input tokens: ['was', 'too'] || next word: long\n",
      "input tokens: ['too', 'long'] || next word: .\n",
      "input tokens: ['long', '.'] || next word: victarion\n",
      "input tokens: ['.', 'victarion'] || next word: took\n",
      "input tokens: ['victarion', 'took'] || next word: men\n",
      "input tokens: ['took', 'men'] || next word: know\n",
      "input tokens: ['men', 'know'] || next word: about\n",
      "input tokens: ['know', 'about'] || next word: day\n",
      "input tokens: ['about', 'day'] || next word: ,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'meanwhile in winterfell , jon and catelyn walk to to his teeth about no notice . those secret whispers . as well . you fell had done or else . your lord father , lord of casterly and the solar behind him . once it is hurting so was too long . victarion took men know about day ,'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(input_sequence,\n",
    "              model=model,\n",
    "              steps=50,\n",
    "              window_size=window_size,\n",
    "              seed=seed,\n",
    "              T=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de196d8",
   "metadata": {},
   "source": [
    "Increasing window size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fed30743",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### window_size: 4\n",
      "Bengio_et_al_word2vec(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=900, out_features=450, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=450, out_features=24114, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "perplexity: 26702.04267301553\n",
      "loss: 10.192495  [   64/1667648]\n",
      "perplexity: 321.5304559221259\n",
      "loss: 5.773092  [640064/1667648]\n",
      "perplexity: 159.68819202574832\n",
      "loss: 5.073223  [1280064/1667648]\n",
      "Test:\n",
      " - Accuracy: 14.1%\n",
      "- Avg loss: 5.682492\n",
      " - Avg perplexity: 390.1127882027954\n",
      "\n",
      "Epoch 500000\n",
      "-------------------------------\n",
      "perplexity: 176.68868855985428\n",
      "loss: 5.174389  [   64/1667648]\n",
      "perplexity: 352.7773414545179\n",
      "loss: 5.865837  [640064/1667648]\n",
      "perplexity: 168.97748996173638\n",
      "loss: 5.129766  [1280064/1667648]\n",
      "Test:\n",
      " - Accuracy: 14.2%\n",
      "- Avg loss: 5.789943\n",
      " - Avg perplexity: 629.3706449895562\n",
      "\n",
      "Epoch 1000000\n",
      "-------------------------------\n",
      "perplexity: 207.7656002683925\n",
      "loss: 5.336411  [   64/1667648]\n",
      "perplexity: 311.43626951131046\n",
      "loss: 5.741195  [640064/1667648]\n",
      "perplexity: 158.6788159085133\n",
      "loss: 5.066882  [1280064/1667648]\n",
      "Test:\n",
      " - Accuracy: 14.1%\n",
      "- Avg loss: 5.888306\n",
      " - Avg perplexity: 789.7406917368905\n",
      "\n",
      "Done!\n",
      "##### window_size: 5\n",
      "Bengio_et_al_word2vec(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=1200, out_features=600, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=600, out_features=24114, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "perplexity: 23822.671314420837\n",
      "loss: 10.078393  [   64/1667647]\n",
      "perplexity: 311.6005586796702\n",
      "loss: 5.741722  [640064/1667647]\n",
      "perplexity: 214.76881038617555\n",
      "loss: 5.369562  [1280064/1667647]\n",
      "Test:\n",
      " - Accuracy: 13.9%\n",
      "- Avg loss: 5.749252\n",
      " - Avg perplexity: 390.18314981052356\n",
      "\n",
      "Epoch 500000\n",
      "-------------------------------\n",
      "perplexity: 322.27152633512424\n",
      "loss: 5.775394  [   64/1667647]\n",
      "perplexity: 306.98137877805726\n",
      "loss: 5.726787  [640064/1667647]\n",
      "perplexity: 191.914789320689\n",
      "loss: 5.257051  [1280064/1667647]\n",
      "Test:\n",
      " - Accuracy: 14.0%\n",
      "- Avg loss: 5.830819\n",
      " - Avg perplexity: 462.3229021965192\n",
      "\n",
      "Epoch 1000000\n",
      "-------------------------------\n",
      "perplexity: 340.6998071326869\n",
      "loss: 5.831002  [   64/1667647]\n",
      "perplexity: 266.3705696434045\n",
      "loss: 5.584888  [640064/1667647]\n",
      "perplexity: 242.57052107413307\n",
      "loss: 5.491292  [1280064/1667647]\n",
      "Test:\n",
      " - Accuracy: 14.0%\n",
      "- Avg loss: 5.898825\n",
      " - Avg perplexity: 498.2475879627044\n",
      "\n",
      "Done!\n",
      "##### window_size: 6\n",
      "Bengio_et_al_word2vec(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=1500, out_features=750, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=750, out_features=24114, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "perplexity: 26637.642945021926\n",
      "loss: 10.190081  [   64/1667646]\n",
      "perplexity: 486.89423953566126\n",
      "loss: 6.188047  [640064/1667646]\n",
      "perplexity: 177.77507079612116\n",
      "loss: 5.180519  [1280064/1667646]\n",
      "Test:\n",
      " - Accuracy: 13.7%\n",
      "- Avg loss: 5.800366\n",
      " - Avg perplexity: 381.9113216826565\n",
      "\n",
      "Epoch 500000\n",
      "-------------------------------\n",
      "perplexity: 322.22066523517367\n",
      "loss: 5.775237  [   64/1667646]\n",
      "perplexity: 256.18977146991773\n",
      "loss: 5.545918  [640064/1667646]\n",
      "perplexity: 130.56061686494783\n",
      "loss: 4.871838  [1280064/1667646]\n",
      "Test:\n",
      " - Accuracy: 13.9%\n",
      "- Avg loss: 5.868078\n",
      " - Avg perplexity: 421.5139633292725\n",
      "\n",
      "Epoch 1000000\n",
      "-------------------------------\n",
      "perplexity: 244.50018845993603\n",
      "loss: 5.499216  [   64/1667646]\n",
      "perplexity: 307.21758074932075\n",
      "loss: 5.727556  [640064/1667646]\n",
      "perplexity: 160.68383658284378\n",
      "loss: 5.079439  [1280064/1667646]\n",
      "Test:\n",
      " - Accuracy: 13.7%\n",
      "- Avg loss: 5.944565\n",
      " - Avg perplexity: 473.630375149745\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model_dict = {3: model}\n",
    "for window_size in [4,5,6]:\n",
    "    X, y = create_dataset_for_Bengio(window_size,\n",
    "                                     tokens,\n",
    "                                     token_embeddings,\n",
    "                                     vocab_to_int)\n",
    "    train_loader, test_loader = split_dataset(x_data=X,\n",
    "                                              y_data=y,\n",
    "                                              train_size=train_size,\n",
    "                                              batch_size=batch_size,\n",
    "                                              seed=seed)\n",
    "    embedding_dim = token_embeddings.shape[1]\n",
    "    hidden_dim = int((window_size-1)*embedding_dim/2)\n",
    "    set_seed(seed)\n",
    "    model_dict[window_size] = Bengio_et_al_word2vec(n=window_size,\n",
    "                                                    embedding_dim=embedding_dim,\n",
    "                                                    hidden_dim=hidden_dim,\n",
    "                                                    vocab_size=len(vocab_to_int)).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model_dict[window_size].parameters(), lr=1e-3)\n",
    "    print(f\"##### window_size: {window_size}\")\n",
    "    print(model_dict[window_size])\n",
    "    \n",
    "    train_and_test(model=model_dict[window_size],\n",
    "                   epochs=epochs,\n",
    "                   train_loader=train_loader,\n",
    "                   test_loader=test_loader,\n",
    "                   loss_fn=loss_fn,\n",
    "                   optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d7630b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial input text: meanwhile in winterfell , jon and catelyn walk to\n",
      "input tokens: ['catelyn', 'walk', 'to'] || next word: the\n",
      "input tokens: ['walk', 'to', 'the'] || next word: king\n",
      "input tokens: ['to', 'the', 'king'] || next word: of\n",
      "input tokens: ['the', 'king', 'of'] || next word: wisdom\n",
      "input tokens: ['king', 'of', 'wisdom'] || next word: men\n",
      "input tokens: ['of', 'wisdom', 'men'] || next word: wife\n",
      "input tokens: ['wisdom', 'men', 'wife'] || next word: ,\n",
      "input tokens: ['men', 'wife', ','] || next word: ned\n",
      "input tokens: ['wife', ',', 'ned'] || next word: snow\n",
      "input tokens: [',', 'ned', 'snow'] || next word: ,\n",
      "input tokens: ['ned', 'snow', ','] || next word: the\n",
      "input tokens: ['snow', ',', 'the'] || next word: truth\n",
      "input tokens: [',', 'the', 'truth'] || next word: door\n",
      "input tokens: ['the', 'truth', 'door'] || next word: .\n",
      "input tokens: ['truth', 'door', '.'] || next word: i\n",
      "input tokens: ['door', '.', 'i'] || next word: shall\n",
      "input tokens: ['.', 'i', 'shall'] || next word: have\n",
      "input tokens: ['i', 'shall', 'have'] || next word: called\n",
      "input tokens: ['shall', 'have', 'called'] || next word: beneath\n",
      "input tokens: ['have', 'called', 'beneath'] || next word: long\n",
      "input tokens: ['called', 'beneath', 'long'] || next word: ,\n",
      "input tokens: ['beneath', 'long', ','] || next word: lord\n",
      "input tokens: ['long', ',', 'lord'] || next word: and\n",
      "input tokens: [',', 'lord', 'and'] || next word: pushed\n",
      "input tokens: ['lord', 'and', 'pushed'] || next word: her\n",
      "input tokens: ['and', 'pushed', 'her'] || next word: father\n",
      "input tokens: ['pushed', 'her', 'father'] || next word: ,\n",
      "input tokens: ['her', 'father', ','] || next word: send\n",
      "input tokens: ['father', ',', 'send'] || next word: the\n",
      "input tokens: [',', 'send', 'the'] || next word: time\n",
      "input tokens: ['send', 'the', 'time'] || next word: they\n",
      "input tokens: ['the', 'time', 'they'] || next word: brothels\n",
      "input tokens: ['time', 'they', 'brothels'] || next word: south\n",
      "input tokens: ['they', 'brothels', 'south'] || next word: .\n",
      "input tokens: ['brothels', 'south', '.'] || next word: arya\n",
      "input tokens: ['south', '.', 'arya'] || next word: did\n",
      "input tokens: ['.', 'arya', 'did'] || next word: not\n",
      "input tokens: ['arya', 'did', 'not'] || next word: presume\n",
      "input tokens: ['did', 'not', 'presume'] || next word: loud\n",
      "input tokens: ['not', 'presume', 'loud'] || next word: ,\n",
      "input tokens: ['presume', 'loud', ','] || next word: but\n",
      "input tokens: ['loud', ',', 'but'] || next word: it\n",
      "input tokens: [',', 'but', 'it'] || next word: was\n",
      "input tokens: ['but', 'it', 'was'] || next word: stabbed\n",
      "input tokens: ['it', 'was', 'stabbed'] || next word: with\n",
      "input tokens: ['was', 'stabbed', 'with'] || next word: walls\n",
      "input tokens: ['stabbed', 'with', 'walls'] || next word: was\n",
      "input tokens: ['with', 'walls', 'was'] || next word: chained\n",
      "input tokens: ['walls', 'was', 'chained'] || next word: as\n",
      "input tokens: ['was', 'chained', 'as'] || next word: a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'meanwhile in winterfell , jon and catelyn walk to the king of wisdom men wife , ned snow , the truth door . i shall have called beneath long , lord and pushed her father , send the time they brothels south . arya did not presume loud , but it was stabbed with walls was chained as a'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(input_sequence,\n",
    "              model=model_dict[4],\n",
    "              steps=50,\n",
    "              window_size=4,\n",
    "              seed=seed,\n",
    "              T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a62f5811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial input text: meanwhile in winterfell , jon and catelyn walk to\n",
      "input tokens: ['and', 'catelyn', 'walk', 'to'] || next word: his\n",
      "input tokens: ['catelyn', 'walk', 'to', 'his'] || next word: and\n",
      "input tokens: ['walk', 'to', 'his', 'and'] || next word: such\n",
      "input tokens: ['to', 'his', 'and', 'such'] || next word: matters\n",
      "input tokens: ['his', 'and', 'such', 'matters'] || next word: rose\n",
      "input tokens: ['and', 'such', 'matters', 'rose'] || next word: south\n",
      "input tokens: ['such', 'matters', 'rose', 'south'] || next word: .\n",
      "input tokens: ['matters', 'rose', 'south', '.'] || next word: these\n",
      "input tokens: ['rose', 'south', '.', 'these'] || next word: muzzle\n",
      "input tokens: ['south', '.', 'these', 'muzzle'] || next word: came\n",
      "input tokens: ['.', 'these', 'muzzle', 'came'] || next word: .\n",
      "input tokens: ['these', 'muzzle', 'came', '.'] || next word: she\n",
      "input tokens: ['muzzle', 'came', '.', 'she'] || next word: meant\n",
      "input tokens: ['came', '.', 'she', 'meant'] || next word: ,\n",
      "input tokens: ['.', 'she', 'meant', ','] || next word: i\n",
      "input tokens: ['she', 'meant', ',', 'i'] || next word: fear\n",
      "input tokens: ['meant', ',', 'i', 'fear'] || next word: to\n",
      "input tokens: [',', 'i', 'fear', 'to'] || next word: give\n",
      "input tokens: ['i', 'fear', 'to', 'give'] || next word: out\n",
      "input tokens: ['fear', 'to', 'give', 'out'] || next word: for\n",
      "input tokens: ['to', 'give', 'out', 'for'] || next word: the\n",
      "input tokens: ['give', 'out', 'for', 'the'] || next word: dagger\n",
      "input tokens: ['out', 'for', 'the', 'dagger'] || next word: .\n",
      "input tokens: ['for', 'the', 'dagger', '.'] || next word: we\n",
      "input tokens: ['the', 'dagger', '.', 'we'] || next word: would\n",
      "input tokens: ['dagger', '.', 'we', 'would'] || next word: make\n",
      "input tokens: ['.', 'we', 'would', 'make'] || next word: to\n",
      "input tokens: ['we', 'would', 'make', 'to'] || next word: remain\n",
      "input tokens: ['would', 'make', 'to', 'remain'] || next word: ,\n",
      "input tokens: ['make', 'to', 'remain', ','] || next word: the\n",
      "input tokens: ['to', 'remain', ',', 'the'] || next word: damp\n",
      "input tokens: ['remain', ',', 'the', 'damp'] || next word: own\n",
      "input tokens: [',', 'the', 'damp', 'own'] || next word: breath\n",
      "input tokens: ['the', 'damp', 'own', 'breath'] || next word: .\n",
      "input tokens: ['damp', 'own', 'breath', '.'] || next word: look\n",
      "input tokens: ['own', 'breath', '.', 'look'] || next word: to\n",
      "input tokens: ['breath', '.', 'look', 'to'] || next word: give\n",
      "input tokens: ['.', 'look', 'to', 'give'] || next word: free\n",
      "input tokens: ['look', 'to', 'give', 'free'] || next word: leagues\n",
      "input tokens: ['to', 'give', 'free', 'leagues'] || next word: .\n",
      "input tokens: ['give', 'free', 'leagues', '.'] || next word: my\n",
      "input tokens: ['free', 'leagues', '.', 'my'] || next word: lady\n",
      "input tokens: ['leagues', '.', 'my', 'lady'] || next word: .\n",
      "input tokens: ['.', 'my', 'lady', '.'] || next word: somehow\n",
      "input tokens: ['my', 'lady', '.', 'somehow'] || next word: frey\n",
      "input tokens: ['lady', '.', 'somehow', 'frey'] || next word: felt\n",
      "input tokens: ['.', 'somehow', 'frey', 'felt'] || next word: on\n",
      "input tokens: ['somehow', 'frey', 'felt', 'on'] || next word: its\n",
      "input tokens: ['frey', 'felt', 'on', 'its'] || next word: stone\n",
      "input tokens: ['felt', 'on', 'its', 'stone'] || next word: .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'meanwhile in winterfell , jon and catelyn walk to his and such matters rose south . these muzzle came . she meant , i fear to give out for the dagger . we would make to remain , the damp own breath . look to give free leagues . my lady . somehow frey felt on its stone .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(input_sequence,\n",
    "              model=model_dict[5],\n",
    "              steps=50,\n",
    "              window_size=5,\n",
    "              seed=seed,\n",
    "              T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d7d9e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial input text: meanwhile in winterfell , jon and catelyn walk to\n",
      "input tokens: ['jon', 'and', 'catelyn', 'walk', 'to'] || next word: his\n",
      "input tokens: ['and', 'catelyn', 'walk', 'to', 'his'] || next word: father\n",
      "input tokens: ['catelyn', 'walk', 'to', 'his', 'father'] || next word: and\n",
      "input tokens: ['walk', 'to', 'his', 'father', 'and'] || next word: buried\n",
      "input tokens: ['to', 'his', 'father', 'and', 'buried'] || next word: off\n",
      "input tokens: ['his', 'father', 'and', 'buried', 'off'] || next word: onto\n",
      "input tokens: ['father', 'and', 'buried', 'off', 'onto'] || next word: the\n",
      "input tokens: ['and', 'buried', 'off', 'onto', 'the'] || next word: fever\n",
      "input tokens: ['buried', 'off', 'onto', 'the', 'fever'] || next word: shouted\n",
      "input tokens: ['off', 'onto', 'the', 'fever', 'shouted'] || next word: him\n",
      "input tokens: ['onto', 'the', 'fever', 'shouted', 'him'] || next word: .\n",
      "input tokens: ['the', 'fever', 'shouted', 'him', '.'] || next word: she\n",
      "input tokens: ['fever', 'shouted', 'him', '.', 'she'] || next word: died\n",
      "input tokens: ['shouted', 'him', '.', 'she', 'died'] || next word: ,\n",
      "input tokens: ['him', '.', 'she', 'died', ','] || next word: i\n",
      "input tokens: ['.', 'she', 'died', ',', 'i'] || next word: dont\n",
      "input tokens: ['she', 'died', ',', 'i', 'dont'] || next word: do\n",
      "input tokens: ['died', ',', 'i', 'dont', 'do'] || next word: was\n",
      "input tokens: [',', 'i', 'dont', 'do', 'was'] || next word: cursed\n",
      "input tokens: ['i', 'dont', 'do', 'was', 'cursed'] || next word: here\n",
      "input tokens: ['dont', 'do', 'was', 'cursed', 'here'] || next word: .\n",
      "input tokens: ['do', 'was', 'cursed', 'here', '.'] || next word: ser\n",
      "input tokens: ['was', 'cursed', 'here', '.', 'ser'] || next word: jaime\n",
      "input tokens: ['cursed', 'here', '.', 'ser', 'jaime'] || next word: thought\n",
      "input tokens: ['here', '.', 'ser', 'jaime', 'thought'] || next word: the\n",
      "input tokens: ['.', 'ser', 'jaime', 'thought', 'the'] || next word: people\n",
      "input tokens: ['ser', 'jaime', 'thought', 'the', 'people'] || next word: was\n",
      "input tokens: ['jaime', 'thought', 'the', 'people', 'was'] || next word: smooth\n",
      "input tokens: ['thought', 'the', 'people', 'was', 'smooth'] || next word: .\n",
      "input tokens: ['the', 'people', 'was', 'smooth', '.'] || next word: the\n",
      "input tokens: ['people', 'was', 'smooth', '.', 'the'] || next word: pardon\n",
      "input tokens: ['was', 'smooth', '.', 'the', 'pardon'] || next word: are\n",
      "input tokens: ['smooth', '.', 'the', 'pardon', 'are'] || next word: angry\n",
      "input tokens: ['.', 'the', 'pardon', 'are', 'angry'] || next word: .\n",
      "input tokens: ['the', 'pardon', 'are', 'angry', '.'] || next word: night\n",
      "input tokens: ['pardon', 'are', 'angry', '.', 'night'] || next word: of\n",
      "input tokens: ['are', 'angry', '.', 'night', 'of'] || next word: me\n",
      "input tokens: ['angry', '.', 'night', 'of', 'me'] || next word: seemed\n",
      "input tokens: ['.', 'night', 'of', 'me', 'seemed'] || next word: another\n",
      "input tokens: ['night', 'of', 'me', 'seemed', 'another'] || next word: claim\n",
      "input tokens: ['of', 'me', 'seemed', 'another', 'claim'] || next word: in\n",
      "input tokens: ['me', 'seemed', 'another', 'claim', 'in'] || next word: the\n",
      "input tokens: ['seemed', 'another', 'claim', 'in', 'the'] || next word: way\n",
      "input tokens: ['another', 'claim', 'in', 'the', 'way'] || next word: right\n",
      "input tokens: ['claim', 'in', 'the', 'way', 'right'] || next word: that\n",
      "input tokens: ['in', 'the', 'way', 'right', 'that'] || next word: big\n",
      "input tokens: ['the', 'way', 'right', 'that', 'big'] || next word: who\n",
      "input tokens: ['way', 'right', 'that', 'big', 'who'] || next word: sniffed\n",
      "input tokens: ['right', 'that', 'big', 'who', 'sniffed'] || next word: on\n",
      "input tokens: ['that', 'big', 'who', 'sniffed', 'on'] || next word: the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'meanwhile in winterfell , jon and catelyn walk to his father and buried off onto the fever shouted him . she died , i dont do was cursed here . ser jaime thought the people was smooth . the pardon are angry . night of me seemed another claim in the way right that big who sniffed on the'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(input_sequence,\n",
    "              model=model_dict[6],\n",
    "              steps=50,\n",
    "              window_size=6,\n",
    "              seed=seed,\n",
    "              T=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c742b",
   "metadata": {},
   "source": [
    "## Letting the model train the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b593386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bengio_et_al(nn.Module):\n",
    "    def __init__(self, n, embedding_dim, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_dim),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((n-1)*embedding_dim, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea1bd",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cce03d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_word2vec(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers,\n",
    "                          nonlinearity='relu',\n",
    "                          batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, embeddings, hidden=None):\n",
    "        outputs, hidden = self.rnn(embeddings, hidden)\n",
    "        logits = self.linear(outputs)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def init_zero_hidden(self, batch_size=1):\n",
    "        if batch_size==0:\n",
    "            return torch.zeros(self.num_layers,\n",
    "                               self.hidden_dim,\n",
    "                               requires_grad=False).to(device)\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers,\n",
    "                               batch_size,\n",
    "                               self.hidden_dim,\n",
    "                               requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be3f551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers,\n",
    "                          nonlinearity='relu',\n",
    "                          batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, hidden=None):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        outputs, hidden = self.rnn(embeddings, hidden)\n",
    "        logits = self.linear(outputs)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def init_zero_hidden(self, batch_size=1):\n",
    "        if batch_size==0:\n",
    "            return torch.zeros(self.num_layers,\n",
    "                               self.hidden_dim,\n",
    "                               requires_grad=False).to(device)\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers,\n",
    "                               batch_size,\n",
    "                               self.hidden_dim,\n",
    "                               requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0ad3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute prediction error for each token in sequence\n",
    "        pred, _ = model(X)\n",
    "        loss = 0\n",
    "        for c in range(y.shape[1]):\n",
    "            loss += loss_fn(pred[:,c], y[:,c])\n",
    "        # divide our loss by the length of the sequence\n",
    "        # to obtain average CE loss for this batch\n",
    "        loss /= y.shape[1]\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"perplexity: {math.exp(loss)}\")\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_RNN(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_perp, test_loss, correct = 0, 0, 0\n",
    "    size = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred, _ = model(X)                \n",
    "            size += y.shape[0]*y.shape[1]\n",
    "            loss = 0\n",
    "            for c in range(y.shape[1]):\n",
    "                loss += loss_fn(pred[:,c], y[:,c]).item()\n",
    "                correct += (pred[:,c].argmax(1) == y[:,c]).type(torch.float).sum().item()\n",
    "                size += 1\n",
    "            loss /= y.shape[1]\n",
    "            test_loss += loss\n",
    "            test_perp += math.exp(loss)\n",
    "    test_loss /= num_batches\n",
    "    test_perp /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test:\\n - Accuracy: {(100*correct):>0.1f}\\n\"\n",
    "          f\"- Avg loss: {test_loss:>8f}\\n\",\n",
    "          f\"- Avg perplexity: {test_perp}\\n\")\n",
    "    \n",
    "def train_and_test_RNN(model, epochs, train_loader, test_loader, loss_fn, optimizer):\n",
    "    for t in range(0, epochs+1, 500000):\n",
    "        print(f\"Epoch {t}\\n-------------------------------\")\n",
    "        train_RNN(train_loader, model, loss_fn, optimizer)\n",
    "        test_RNN(test_loader, model, loss_fn)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "423cc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_for_RNN(length_of_sequences, tokens, token_embeddings, vocab_to_int):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(0, len(tokens)-length_of_sequences, length_of_sequences):\n",
    "        if token_embeddings is None:\n",
    "            # obtain token-ids for the next i:i+length_of_sequences words\n",
    "            X.append([vocab_to_int[token] for token in tokens[i:(i+length_of_sequences)]])\n",
    "        else:   \n",
    "            # obtain word embeddings for the next i:i+length_of_sequences words\n",
    "            X.append(token_embeddings[i:(i+length_of_sequences)])\n",
    "        # obtain token_ids for the next (i+1):(i+length_of_sequences+1) words\n",
    "        # we're sequentially predicting with an RNN\n",
    "        y.append([vocab_to_int[token]\n",
    "                  for token in tokens[(i+1):(i+length_of_sequences+1)]])\n",
    "    if token_embeddings is None:\n",
    "        return torch.tensor(X), torch.tensor(y)\n",
    "    else:\n",
    "        return torch.stack(X), torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b327eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_word2vec(\n",
      "  (rnn): RNN(300, 200, batch_first=True)\n",
      "  (linear): Linear(in_features=200, out_features=24114, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "X,y = create_dataset_for_RNN(50, tokens, token_embeddings, vocab_to_int)\n",
    "train_loader, test_loader = split_dataset(x_data=X,\n",
    "                                          y_data=y,\n",
    "                                          train_size=train_size,\n",
    "                                          batch_size=batch_size,\n",
    "                                          seed=seed)\n",
    "\n",
    "embedding_dim = 300\n",
    "hidden_dim = 200\n",
    "set_seed(seed)\n",
    "RNN_w2v = RNN_word2vec(embedding_dim=embedding_dim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       num_layers=1,\n",
    "                       vocab_size=len(vocab_to_int)).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(RNN_w2v.parameters(), lr=1e-3)\n",
    "print(RNN_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "291ab7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41691, 50, 300])\n",
      "torch.Size([41691, 50])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93c71f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "perplexity: 59205.54517157895\n",
      "loss: 10.988770  [   64/33352]\n",
      "Test:\n",
      " - Accuracy: 16.2%\n",
      "- Avg loss: 5.098456\n",
      " - Avg perplexity: 164.160492856811\n",
      "\n",
      "Epoch 500000\n",
      "-------------------------------\n",
      "perplexity: 145.9900732012472\n",
      "loss: 4.983539  [   64/33352]\n",
      "Test:\n",
      " - Accuracy: 17.0%\n",
      "- Avg loss: 4.985477\n",
      " - Avg perplexity: 146.64926832916578\n",
      "\n",
      "Epoch 1000000\n",
      "-------------------------------\n",
      "perplexity: 114.53178193429657\n",
      "loss: 4.740852  [   64/33352]\n",
      "Test:\n",
      " - Accuracy: 17.5%\n",
      "- Avg loss: 4.979248\n",
      " - Avg perplexity: 145.8150903935469\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_and_test_RNN(model=RNN_w2v,\n",
    "                   epochs=epochs,\n",
    "                   train_loader=train_loader,\n",
    "                   test_loader=test_loader,\n",
    "                   loss_fn=loss_fn,\n",
    "                   optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5d639a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = create_dataset_for_RNN(50, tokens, None, vocab_to_int)\n",
    "train_loader, test_loader = split_dataset(x_data=X,\n",
    "                                          y_data=y,\n",
    "                                          train_size=train_size,\n",
    "                                          batch_size=batch_size,\n",
    "                                          seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4838c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([41691, 50])\n",
      "torch.Size([41691, 50])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5357cdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11030,     2, 13332,  1596,  1135,   187,     2,   821,     1,     4,\n",
       "          111,  3378,    16,  4789,   375,     2, 12067,     6,   905,    72,\n",
       "            4,  1069,    13,     2,   912,     3,  1031,   485,     0,     2,\n",
       "          179,   253,    28,     2,  7013,  4535,   467,     8,  1244,     0,\n",
       "           15,     9,   105,     2,   993,   124,     1,   155,    99,  2124])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df56e4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2, 13332,  1596,  1135,   187,     2,   821,     1,     4,   111,\n",
       "         3378,    16,  4789,   375,     2, 12067,     6,   905,    72,     4,\n",
       "         1069,    13,     2,   912,     3,  1031,   485,     0,     2,   179,\n",
       "          253,    28,     2,  7013,  4535,   467,     8,  1244,     0,    15,\n",
       "            9,   105,     2,   993,   124,     1,   155,    99,  2124,     0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96b383fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(24114, 300)\n",
      "  (rnn): RNN(300, 200, batch_first=True)\n",
      "  (linear): Linear(in_features=200, out_features=24114, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "RNNLM = RNN(embedding_dim=embedding_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=1,\n",
    "            vocab_size=len(vocab_to_int)).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(RNNLM.parameters(), lr=1e-3)\n",
    "print(RNNLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "905d552d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "perplexity: 26385.820603912714\n",
      "loss: 10.180582  [   64/33352]\n",
      "Test:\n",
      " - Accuracy: 15.7%\n",
      "- Avg loss: 5.169092\n",
      " - Avg perplexity: 176.17416807423493\n",
      "\n",
      "Epoch 500000\n",
      "-------------------------------\n",
      "perplexity: 158.30894824662178\n",
      "loss: 5.064548  [   64/33352]\n",
      "Test:\n",
      " - Accuracy: 17.1%\n",
      "- Avg loss: 4.984224\n",
      " - Avg perplexity: 146.45302120152473\n",
      "\n",
      "Epoch 1000000\n",
      "-------------------------------\n",
      "perplexity: 117.72990259179983\n",
      "loss: 4.768393  [   64/33352]\n",
      "Test:\n",
      " - Accuracy: 17.9%\n",
      "- Avg loss: 4.916634\n",
      " - Avg perplexity: 136.9206951957755\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_and_test_RNN(model=RNNLM,\n",
    "                   epochs=epochs,\n",
    "                   train_loader=train_loader,\n",
    "                   test_loader=test_loader,\n",
    "                   loss_fn=loss_fn,\n",
    "                   optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73070cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_RNN(input,\n",
    "                      model,\n",
    "                      steps,\n",
    "                      word2vec,\n",
    "                      lstm=False,\n",
    "                      seed=42,\n",
    "                      T=1):\n",
    "    set_seed(seed)\n",
    "    device = \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    text = clean_text(input, remove_punct)\n",
    "    print(f\"initial input text: {text}\")\n",
    "    text_tokens = text.split(' ')\n",
    "    if word2vec:\n",
    "        docs = list(nlp.pipe(text_tokens))\n",
    "        X = torch.tensor([doc.vector for doc in docs]).to(device)\n",
    "    else:\n",
    "        X = torch.tensor([vocab_to_int[token] for token in text_tokens])\n",
    "    softmax = nn.Softmax(dim=0)\n",
    "    with torch.no_grad():\n",
    "        hidden = torch.zeros(model.num_layers,\n",
    "                             model.hidden_dim,\n",
    "                             requires_grad=False).to(device)\n",
    "        if lstm:\n",
    "            context = torch.zeros(model.num_layers,\n",
    "                                  model.hidden_dim,\n",
    "                                  requires_grad=False).to(device)\n",
    "        for i in range(steps):\n",
    "            if lstm:\n",
    "                pred, (hidden, context) = model(X, (hidden, context))\n",
    "            else:\n",
    "                pred, hidden = model(X, hidden)\n",
    "            prob = torch.pow(softmax(pred[-1]), 1/T)\n",
    "            prob = prob/prob.sum()\n",
    "            word = int_to_vocab[random.choices(list(range(len(prob))), weights=prob)[0]]\n",
    "            print(f\"next word: {word}\")\n",
    "            text_tokens.append(word)\n",
    "            if word2vec:\n",
    "                X = torch.tensor([nlp(word).vector]).to(device)\n",
    "            else:\n",
    "                X = torch.tensor([vocab_to_int[word]])\n",
    "    return ' '.join(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f320145c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial input text: meanwhile in winterfell , jon and catelyn walk to\n",
      "next word: the\n",
      "next word: wall\n",
      "next word: ,\n",
      "next word: merchants\n",
      "next word: feet\n",
      "next word: dancing\n",
      "next word: and\n",
      "next word: thumped\n",
      "next word: water\n",
      "next word: with\n",
      "next word: the\n",
      "next word: common\n",
      "next word: sisters\n",
      "next word: .\n",
      "next word: her\n",
      "next word: mind\n",
      "next word: was\n",
      "next word: small\n",
      "next word: light\n",
      "next word: outside\n",
      "next word: .\n",
      "next word: they\n",
      "next word: had\n",
      "next word: gone\n",
      "next word: to\n",
      "next word: battle\n",
      "next word: ,\n",
      "next word: since\n",
      "next word: the\n",
      "next word: king\n",
      "next word: sat\n",
      "next word: up\n",
      "next word: before\n",
      "next word: the\n",
      "next word: cave\n",
      "next word: and\n",
      "next word: pulled\n",
      "next word: hams\n",
      "next word: through\n",
      "next word: the\n",
      "next word: doors\n",
      "next word: .\n",
      "next word: the\n",
      "next word: deserter\n",
      "next word: rose\n",
      "next word: wide\n",
      "next word: at\n",
      "next word: bay\n",
      "next word: as\n",
      "next word: a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'meanwhile in winterfell , jon and catelyn walk to the wall , merchants feet dancing and thumped water with the common sisters . her mind was small light outside . they had gone to battle , since the king sat up before the cave and pulled hams through the doors . the deserter rose wide at bay as a'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_RNN(input_sequence,\n",
    "                  model=RNN_w2v,\n",
    "                  steps=50,\n",
    "                  word2vec=True,\n",
    "                  seed=seed,\n",
    "                  T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8d7202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial input text: meanwhile in winterfell , jon and catelyn walk to\n",
      "next word: the\n",
      "next word: gods\n",
      "next word: of\n",
      "next word: bushy\n",
      "next word: cruel\n",
      "next word: consumed\n",
      "next word: ,\n",
      "next word: drinking\n",
      "next word: insisted\n",
      "next word: that\n",
      "next word: the\n",
      "next word: free\n",
      "next word: folk\n",
      "next word: had\n",
      "next word: only\n",
      "next word: lifted\n",
      "next word: his\n",
      "next word: teeth\n",
      "next word: ;\n",
      "next word: instead\n",
      "next word: the\n",
      "next word: twins\n",
      "next word: ,\n",
      "next word: an\n",
      "next word: eye\n",
      "next word: see\n",
      "next word: to\n",
      "next word: cross\n",
      "next word: the\n",
      "next word: way\n",
      "next word: down\n",
      "next word: to\n",
      "next word: lord\n",
      "next word: snow\n",
      "next word: ,\n",
      "next word: that\n",
      "next word: night\n",
      "next word: soil\n",
      "next word: every\n",
      "next word: time\n",
      "next word: to\n",
      "next word: know\n",
      "next word: .\n",
      "next word: shouldnt\n",
      "next word: be\n",
      "next word: poured\n",
      "next word: off\n",
      "next word: such\n",
      "next word: coming\n",
      "next word: .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'meanwhile in winterfell , jon and catelyn walk to the gods of bushy cruel consumed , drinking insisted that the free folk had only lifted his teeth ; instead the twins , an eye see to cross the way down to lord snow , that night soil every time to know . shouldnt be poured off such coming .'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_RNN(input_sequence,\n",
    "                  model=RNNLM,\n",
    "                  steps=50,\n",
    "                  word2vec=False,\n",
    "                  seed=seed,\n",
    "                  T=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb138f2",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de729369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers,\n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, hidden_and_context=None):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        outputs, hidden_and_context = self.lstm(embeddings, hidden_and_context)\n",
    "        logits = self.linear(outputs)\n",
    "        return logits, hidden_and_context\n",
    "    \n",
    "    def init_zero_hidden(self, batch_size=1):\n",
    "        if batch_size==0:\n",
    "            return torch.zeros(self.num_layers,\n",
    "                               self.hidden_dim,\n",
    "                               requires_grad=False).to(device)\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers,\n",
    "                               batch_size,\n",
    "                               self.hidden_dim,\n",
    "                               requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2350e724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (embedding): Embedding(24114, 300)\n",
      "  (lstm): LSTM(300, 200, batch_first=True)\n",
      "  (linear): Linear(in_features=200, out_features=24114, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "LSTMLM = LSTM(embedding_dim=embedding_dim,\n",
    "              hidden_dim=hidden_dim,\n",
    "              num_layers=1,\n",
    "              vocab_size=len(vocab_to_int)).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(LSTMLM.parameters(), lr=1e-3)\n",
    "print(LSTMLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7cd89aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "perplexity: 24458.742656332503\n",
      "loss: 10.104743  [   64/33352]\n",
      "Test:\n",
      " - Accuracy: 14.0%\n",
      "- Avg loss: 5.487979\n",
      " - Avg perplexity: 242.37346972074675\n",
      "\n",
      "Epoch 500000\n",
      "-------------------------------\n",
      "perplexity: 257.6838623382885\n",
      "loss: 5.551733  [   64/33352]\n",
      "Test:\n",
      " - Accuracy: 15.7%\n",
      "- Avg loss: 5.218495\n",
      " - Avg perplexity: 185.10562069915218\n",
      "\n",
      "Epoch 1000000\n",
      "-------------------------------\n",
      "perplexity: 186.11408468509555\n",
      "loss: 5.226360  [   64/33352]\n",
      "Test:\n",
      " - Accuracy: 16.6%\n",
      "- Avg loss: 5.076339\n",
      " - Avg perplexity: 160.5536762701723\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_and_test_RNN(model=LSTMLM,\n",
    "                   epochs=epochs,\n",
    "                   train_loader=train_loader,\n",
    "                   test_loader=test_loader,\n",
    "                   loss_fn=loss_fn,\n",
    "                   optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f31e0673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial input text: meanwhile in winterfell , jon and catelyn walk to\n",
      "next word: the\n",
      "next word: wall\n",
      "next word: ,\n",
      "next word: awkward\n",
      "next word: two\n",
      "next word: afloat\n",
      "next word: .\n",
      "next word: perhaps\n",
      "next word: still\n",
      "next word: raised\n",
      "next word: the\n",
      "next word: blade\n",
      "next word: at\n",
      "next word: the\n",
      "next word: word\n",
      "next word: of\n",
      "next word: his\n",
      "next word: lips\n",
      "next word: as\n",
      "next word: shed\n",
      "next word: a\n",
      "next word: whole\n",
      "next word: -\n",
      "next word: faced\n",
      "next word: and\n",
      "next word: aegon\n",
      "next word: ,\n",
      "next word: sniffed\n",
      "next word: ,\n",
      "next word: the\n",
      "next word: lantern\n",
      "next word: left\n",
      "next word: himself\n",
      "next word: .\n",
      "next word: jaime\n",
      "next word: thought\n",
      "next word: of\n",
      "next word: garnet\n",
      "next word: screaming\n",
      "next word: .\n",
      "next word: is\n",
      "next word: it\n",
      "next word: .\n",
      "next word: penny\n",
      "next word: frey\n",
      "next word: remained\n",
      "next word: from\n",
      "next word: throw\n",
      "next word: into\n",
      "next word: the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'meanwhile in winterfell , jon and catelyn walk to the wall , awkward two afloat . perhaps still raised the blade at the word of his lips as shed a whole - faced and aegon , sniffed , the lantern left himself . jaime thought of garnet screaming . is it . penny frey remained from throw into the'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_RNN(input_sequence,\n",
    "                  model=LSTMLM,\n",
    "                  steps=50,\n",
    "                  word2vec=False,\n",
    "                  lstm=True,\n",
    "                  seed=seed,\n",
    "                  T=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hut23-robots-in-disguise",
   "language": "python",
   "name": "hut23-robots-in-disguise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
