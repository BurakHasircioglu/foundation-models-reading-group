{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# dataset downloader from https://www.kaggle.com/c/learn-ai-bbc/data\n",
    "df = pd.read_csv(\"data/BBC News Train.csv\")\n",
    "\n",
    "df = df.head(200)\n",
    "\n",
    "corpus = \" \".join(df['Text'].dropna().to_list())\n",
    "\n",
    "# Preprocess the corpus by tokenizing and lowercasing the text\n",
    "tokens = nltk.word_tokenize(corpus.lower())\n",
    "\n",
    "# Generate a list of all 4-grams in the corpus\n",
    "ngrams = list(nltk.ngrams(tokens, 4))\n",
    "\n",
    "# Create a dictionary to store the n-grams and their frequency counts\n",
    "ngram_counts = Counter(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word given a list of 4 words\n",
    "def predict_next_word(words):\n",
    "    # Find all 4-grams that match the input words\n",
    "    matches = [ngram for ngram in ngrams if list(ngram[:-1]) == words[1:]]\n",
    "    if len(matches) == 0:\n",
    "        # No matches found, return None\n",
    "        return None\n",
    "    \n",
    "    # Create a dictionary to store the possible next words and their frequency counts\n",
    "    next_word_counts = Counter([ngram[-1] for ngram in matches])\n",
    "    \n",
    "    # Compute the total count of all possible next words\n",
    "    total_count = sum(next_word_counts.values())\n",
    "    \n",
    "    # Create a dictionary to store the probabilities of each possible next word\n",
    "    next_word_probs = {word: count / total_count for word, count in next_word_counts.items()}\n",
    "    \n",
    "    # Use random.choices to sample a word from the probability distribution\n",
    "    next_word = random.choices(list(next_word_probs.keys()), weights=list(next_word_probs.values()))[0]\n",
    "    \n",
    "    # Return the sampled next word\n",
    "    return next_word\n",
    "\n",
    "def generate_text(input,seq_length):\n",
    "    output = input.copy()\n",
    "\n",
    "    for i in range(seq_length):\n",
    "        # Predict the next word\n",
    "        next_word = predict_next_word(input)\n",
    "        # if there's no prediction, return the output as it is at the point\n",
    "        if next_word is None:\n",
    "            return \" \".join(output)\n",
    "        output.append(next_word)\n",
    "        # Add the new word to the list of words\n",
    "        input.append(next_word)\n",
    "        # Remove the first word from the list of words\n",
    "        input = input[1:]\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the new home secretary had time to think again . he is optimistic however . he thinks that the alert retrieval cache . the idea is to use open-source software - software can be used . but camera phone fears have not dampened the manufacturers profits . according to the financial ombudsman service .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "words = [\"the\", \"new\", \"home\", \"secretary\"]\n",
    "\n",
    "generate_text(words,50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
