{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_splitter import split_text_into_sentences\n",
    "\n",
    "def sentences_split(text: str) -> list:\n",
    "    sentences = split_text_into_sentences(text, language=\"en\")\n",
    "    return sentences\n",
    "\n",
    "# dataset downloader from https://www.kaggle.com/c/learn-ai-bbc/data\n",
    "df = pd.read_csv(\"data/BBC News Train.csv\")\n",
    "\n",
    "df = df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "['german', 'business', 'confidence', 'slides', 'german', 'business', 'confidence', 'fell', 'in', 'february', 'knocking', 'hopes', 'of', 'a', 'speedy', 'recovery', 'in', 'europe', 's', 'largest', 'economy.', 'munich-based', 'research', 'institute', 'ifo', 'said', 'that', 'its', 'confidence', 'index', 'fell', 'to', '95.5', 'in', 'february', 'from', '97.5', 'in', 'january', 'its', 'first', 'decline', 'in', 'three', 'months.', 'the', 'study', 'found', 'that', 'the', 'outlook', 'in', 'both', 'the', 'manufacturing', 'and', 'retail', 'sectors', 'had', 'worsened.', 'observers', 'had', 'been', 'hoping', 'that', 'a', 'more', 'confident', 'business', 'sector', 'would', 'signal', 'that', 'economic', 'activity', 'was', 'picking', 'up.', 'we', 're', 'surprised', 'that', 'the', 'ifo', 'index', 'has', 'taken', 'such', 'a', 'knock', 'said', 'dz', 'bank', 'economist', 'bernd', 'weidensteiner.', 'the', 'main', 'reason', 'is', 'probably', 'that', 'the', 'domestic', 'economy', 'is', 'still', 'weak', 'particularly', 'in', 'the', 'retail', 'trade.', 'economy', 'and', 'labour', 'minister', 'wolfgang', 'clement', 'called', 'the', 'dip', 'in', 'february', 's', 'ifo', 'confidence', 'figure', 'a', 'very', 'mild', 'decline', '.', 'he', 'said', 'that', 'despite', 'the', 'retreat', 'the', 'index', 'remained', 'at', 'a', 'relatively', 'high', 'level', 'and', 'that', 'he', 'expected', 'a', 'modest', 'economic', 'upswing', 'to', 'continue.', 'germany', 's', 'economy', 'grew', '1.6%', 'last', 'year', 'after', 'shrinking', 'in', '2003.', 'however', 'the', 'economy', 'contracted', 'by', '0.2%', 'during', 'the', 'last', 'three', 'months', 'of', '2004', 'mainly', 'due', 'to', 'the', 'reluctance', 'of', 'consumers', 'to', 'spend.', 'latest', 'indications', 'are', 'that', 'growth', 'is', 'still', 'proving', 'elusive', 'and', 'ifo', 'president', 'hans-werner', 'sinn', 'said', 'any', 'improvement', 'in', 'german', 'domestic', 'demand', 'was', 'sluggish.', 'exports', 'had', 'kept', 'things', 'going', 'during', 'the', 'first', 'half', 'of', '2004', 'but', 'demand', 'for', 'exports', 'was', 'then', 'hit', 'as', 'the', 'value', 'of', 'the', 'euro', 'hit', 'record', 'levels', 'making', 'german', 'products', 'less', 'competitive', 'overseas.', 'on', 'top', 'of', 'that', 'the', 'unemployment', 'rate', 'has', 'been', 'stuck', 'at', 'close', 'to', '10%', 'and', 'manufacturing', 'firms', 'including', 'daimlerchrysler', 'siemens', 'and', 'volkswagen', 'have', 'been', 'negotiating', 'with', 'unions', 'over', 'cost', 'cutting', 'measures.', 'analysts', 'said', 'that', 'the', 'ifo', 'figures', 'and', 'germany', 's', 'continuing', 'problems', 'may', 'delay', 'an', 'interest', 'rate', 'rise', 'by', 'the', 'european', 'central', 'bank.', 'eurozone', 'interest', 'rates', 'are', 'at', '2%', 'but', 'comments', 'from', 'senior', 'officials', 'have', 'recently', 'focused', 'on', 'the', 'threat', 'of', 'inflation', 'prompting', 'fears', 'that', 'interest', 'rates', 'may', 'rise.']\n"
     ]
    }
   ],
   "source": [
    "texts = df['Text'].dropna().to_list()\n",
    "sentences = [sent.split(\" \") for text in texts for sent in sentences_split(text)]\n",
    "print (len(sentences))\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input, seq_length, model):\n",
    "    seq = input.copy()\n",
    "    output = input.copy()\n",
    "    for i in range(seq_length):\n",
    "        words = model.predict_output_word(input, topn=len(output)+1)\n",
    "        \n",
    "        # excluding repetitions\n",
    "        word = [word[0] for word in words if word[0] not in output][0]\n",
    "\n",
    "        output.append(word)\n",
    "        seq.append(word)\n",
    "\n",
    "        #keeping the last 4 words in the input\n",
    "        seq.remove(seq[0])\n",
    "\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Size of the word vectors\n",
    "vector_size = 50\n",
    "\n",
    "# Minimum count for words to be included in the vocabulary\n",
    "min_count = 1\n",
    "\n",
    "# Number of threads to use for training the model\n",
    "workers = 3\n",
    "\n",
    "# Context window size\n",
    "window = 5\n",
    "\n",
    "# Number of iterations over the corpus\n",
    "epochs = 100\n",
    "\n",
    "# Use skip-gram model with negative sampling and softmax activation\n",
    "sg = 1\n",
    "ns = 5\n",
    "alpha = 0.03\n",
    "\n",
    "basic_w2v = Word2Vec(sentences,vector_size=vector_size, window=window, min_count=min_count, workers=workers, epochs=epochs, sg=sg, negative=ns, alpha=alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the new home secretary office powers. id broom called policing. association charles sweep mps'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input = [\"the\", \"new\", \"home\", \"secretary\"]\n",
    "seq_length = 10\n",
    "\n",
    "generate_text(text_input, seq_length, basic_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5935504, 7467500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# https://datascience.stackexchange.com/questions/97568/fine-tuning-pre-trained-word2vec-model-with-gensim-4-0\n",
    "\n",
    "finetunded_w2v = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg, negative=ns, alpha=alpha)\n",
    "\n",
    "finetunded_w2v.build_vocab(sentences)\n",
    "\n",
    "finetunded_w2v.wv.vectors_lockf = np.ones(len(finetunded_w2v.wv))\n",
    "\n",
    "# download glove.6B.50d.word2vec.txt from https://nlp.stanford.edu/projects/glove/\n",
    "# convert glove to word2vec format following: https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "finetunded_w2v.wv.intersect_word2vec_format('glove.6B.50d.word2vec.txt', binary=False)\n",
    "\n",
    "finetunded_w2v.train(sentences, total_examples=len(sentences),  epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the new home secretary zealand. broom office general minister powers. chairman executive stadium. election'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(text_input, seq_length, finetunded_w2v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
