# Transformers Reading Group

Public repo for the [Research Engineering Team](https://www.turing.ac.uk/research-engineering)'s reading group on Transformers.

Follow `#hut23-robots-in-disguise` on Slack for the most recent updates.

## Overview

The group meets every <b>2 weeks on Mondays at 11am</b>. Everyone is welcome to join! If you have any questions email [Ryan](mailto:rchan@turing.ac.uk).

The plan is to learn about transformer models (assuming little knowledge of deep learning and NLP) with the end goal of implementing a transformers model from scratch.

Please add suggestions and emoji preferences to the [list of proposed topics](https://hackmd.io/NILcoBk1QquVNkDR6dbIBA) on HackMD.

## Schedule

|Date | Topic | Room | Lead | Notes |
| --- | ----- | ---- | ---- | ----- |
| 20/03/23 | Introduction to word embeddings and language modelling ([Slides](https://docs.google.com/presentation/d/1i56HKtjcdQFTxacxsjgya_giDx8Mv1xZn-IDNc_mK8I/edit?usp=sharing)) | David Blackwell | [Fede Nanni](https://github.com/fedenanni) | N/A |
| 03/04/23 | Deep Learning Basics | David Blackwell | [Phil Swatton](https://github.com/philswatton) & [Jack Roberts](https://github.com/jack89roberts) | N/A |
| 17/04/23 | Sequence-to-sequence models part I (RNN) | David Blackwell | [Ryan Chan](https://github.com/rchan26) | N/A |
| 01/05/23 | Sequence-to-sequence models part II (LSTM) | David Blackwell | N/A | N/A |
| 15/05/23 | Hands-on RNN/LSTM session | David Blackwell | N/A | N/A |
| 29/05/23 | Attention | David Blackwell | N/A | N/A |

## Material for sessions

- Introduction to Word Embeddings and Language modelling
  - Main
    - [Don't Count, Predict! paper](https://aclanthology.org/P14-1023.pdf)
    - [Word Embeddings (1)](https://www.ruder.io/word-embeddings-1/)
    - [Word Embeddings (2)](https://www.ruder.io/word-embeddings-softmax/)
    - [Word Embeddings (3)](https://www.ruder.io/secret-word2vec/)
    - [Brief History of NLP (part 1)](https://medium.com/@antoine.louis/a-brief-history-of-natural-language-processing-part-1-ffbcb937ebce)
    - [Brief History of NLP (part 2)](https://medium.com/@antoine.louis/a-brief-history-of-natural-language-processing-part-2-f5e575e8e37)
  - Extra
    - [Deep Learning, NLP and Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
    - [Stanford NLP with Deep Learning (Lecture 1 - Intro & Word Vectors)](https://youtu.be/rmVRLeJRkl4)
    - [Speech and Language Processing - Chapter 6: Vector Semantics and Embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf)
    - [Stanford Large Language Models (Lecture 1 - Introduction)](https://stanford-cs324.github.io/winter2022/lectures/introduction/)
  
- RNNs / LSTMs
  - [Speech and Language Processing - Chapter 7: Neural Networks and Neural Language Models](https://web.stanford.edu/~jurafsky/slp3/7.pdf)
  - [Speech and Language Processing - Chapter 9: RNNs and LSTMs](https://web.stanford.edu/~jurafsky/slp3/9.pdf)
  - [Stanford NLP with Deep Learning (Lecture 5 - RNNs)](https://youtu.be/PLryWeHPcBs)
  - [Stanford NLP with Deep Learning (Lecture 6 - Simple LSTM RNNs)](https://youtu.be/0LixFSa7yts)
  
- Attention
  - [Michigan Deep Learning for Comp Vis (Lecture 13 - Attention)](https://www.youtube.com/watch?v=YAgjfMR9R_M)
  - [Stanford NLP with Deep Learning (Lecture 7 - Translation, Seq2Seq, Attention)](https://youtu.be/wzfWHP6SXxY)
  
